"""
Plan Node - ì¼ì • ê³„íš ë…¸ë“œ

LangGraph ê¸°ë°˜ì˜ ì¼ì • ê³„íš ìˆ˜ë¦½ ë…¸ë“œì…ë‹ˆë‹¤.
"""

from typing import Dict, Any
import logging
import re
import os
from datetime import datetime, timedelta
from models import State, Task, ScheduleData
from langchain_openai import ChatOpenAI
from langchain.tools import Tool
from langchain_core.prompts import PromptTemplate

logger = logging.getLogger("node.plan")


async def plan_node(state: State) -> State:
    """
    ì¼ì • ê³„íš ìˆ˜ë¦½ ë…¸ë“œ
    
    Args:
        state (State): í˜„ì¬ ìƒíƒœ
        
    Returns:
        State: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    print("\n\n============= PLAN NODE ==============\n")
    
    logger.info("Plan node processing started")
    
    try:
        # í˜„ì¬ ì‘ì—… í™•ì¸
        current_task = state.get("current_task")
        if not current_task or current_task.agent != "plan_agent":
            logger.warning("Plan node called without proper task assignment")
            return state
        
        user_id = state.get("user_id", 1)
        user_request = state.get("user_request", "")
        
        # ì¼ì • ê³„íš ìˆ˜ë¦½
        schedule_plan = await create_schedule_plan(user_id, user_request, state)
        
        # ì¼ì • ë°ì´í„° ì—…ë°ì´íŠ¸
        schedule_data = ScheduleData(
            schedule_id=schedule_plan["schedule_id"],
            tasks=schedule_plan["tasks"],
            time_blocks=schedule_plan["time_blocks"],
            constraints=schedule_plan["constraints"],
            efficiency_score=schedule_plan["efficiency_score"],
            conflicts=schedule_plan["conflicts"]
        )
        
        # ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        messages = state.get("messages", [])
        plan_message = {
            "role": "assistant",
            "content": f"[Plan Agent] ì¼ì • ê³„íš ì™„ë£Œ: {len(schedule_plan['time_blocks'])}ê°œ ë¸”ë¡ ìƒì„±",
            "timestamp": datetime.now().isoformat(),
            "agent": "plan_agent"
        }
        messages.append(plan_message)
        
        # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
        task_history = state.get("task_history", [])
        if task_history:
            task_history[-1].done = True
            task_history[-1].done_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # AI ì¶”ì²œ ìƒì„±
        ai_recommendation = await generate_plan_recommendation(schedule_plan, state)
        
        # AI ì‘ë‹µì„ stateì— ì¶”ê°€
        ai_response = f"ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\n\n"
        ai_response += f"ğŸ“… ì¼ì •: {schedule_plan['tasks'][0]['title']}\n"
        ai_response += f"â° ì‹œê°„: {schedule_plan['tasks'][0]['deadline']}\n"
        ai_response += f"â±ï¸ ì†Œìš”ì‹œê°„: {schedule_plan['tasks'][0]['duration']}ë¶„\n"
        ai_response += f"ğŸ¯ ìš°ì„ ìˆœìœ„: {schedule_plan['tasks'][0]['priority']}/10\n\n"
        ai_response += ai_recommendation
        
        logger.info("Plan node processing completed successfully")
        
        return {
            **state,
            "messages": messages,
            "schedule_data": schedule_data,
            "task_history": task_history,
            "ai_recommendation": ai_recommendation,
            "ai_response": ai_response,
            "system_status": "schedule_plan_completed"
        }
        
    except Exception as e:
        logger.error(f"Error in plan node: {str(e)}")
        error_messages = state.get("error_messages", [])
        error_messages.append(f"Plan node error: {str(e)}")
        
        return {
            **state,
            "error_messages": error_messages,
            "system_status": "error"
        }


async def create_schedule_plan(user_id: int, user_request: str, state: State) -> Dict[str, Any]:
    """
    ì¼ì • ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        user_id (int): ì‚¬ìš©ì ID
        user_request (str): ì‚¬ìš©ì ìš”ì²­
        state (State): í˜„ì¬ ìƒíƒœ
        
    Returns:
        Dict[str, Any]: ì¼ì • ê³„íš ê²°ê³¼
    """
    # ScheduleToolsë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì¼ì • ìƒì„±
    from tools import ScheduleTools
    schedule_tools = ScheduleTools()
    
    schedule_id = f"schedule_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # AIê°€ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ titleê³¼ description ìƒì„±
    from langchain_openai import ChatOpenAI
    import os
    
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0.3
    )
    
    # ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ì„ ì œê±°í•˜ê³  ìˆœìˆ˜í•œ ì‚¬ìš©ì ìš”ì²­ë§Œ ì¶”ì¶œ
    pure_user_request = user_request
    if "[ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­]" in user_request and "[ì‚¬ìš©ì ì…ë ¥]" in user_request:
        # [ì‚¬ìš©ì ì…ë ¥] ì´í›„ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
        user_input_start = user_request.find("[ì‚¬ìš©ì ì…ë ¥]")
        if user_input_start != -1:
            pure_user_request = user_request[user_input_start + len("[ì‚¬ìš©ì ì…ë ¥]"):].strip()
    
    # AIê°€ ì¼ì • ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ titleê³¼ description ìƒì„±
    analysis_prompt = f"""
    ì‚¬ìš©ì ìš”ì²­: "{pure_user_request}"
    
    ì´ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì¼ì •ì„ ìƒì„±í•  ë•Œ:
    1. title: ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì œëª© (ì˜ˆ: "ìš´ë™", "ë¯¸íŒ…", "í”„ë¡œì íŠ¸ ì‘ì—…")
    2. description: ì‚¬ìš©ìì˜ ìš”ì²­ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì •ë¦¬í•œ ì„¤ëª…
    
    JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
    {{
        "title": "ê°„ë‹¨í•œ ì œëª©",
        "description": "ì‚¬ìš©ì ìš”ì²­ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì •ë¦¬í•œ ì„¤ëª…"
    }}
    """
    
    analysis_result = llm.invoke(analysis_prompt)
    analysis_text = analysis_result.content.strip()
    
    # JSON íŒŒì‹± ì‹œë„
    try:
        import json
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "{" in analysis_text and "}" in analysis_text:
            json_start = analysis_text.find("{")
            json_end = analysis_text.rfind("}") + 1
            json_text = analysis_text[json_start:json_end]
            analysis_data = json.loads(json_text)
            title = analysis_data.get("title", "ì¼ì •")
            description = analysis_data.get("description", user_request)
        else:
            title = "ì¼ì •"
            description = user_request
    except:
        title = "ì¼ì •"
        description = user_request
    
    # ì‚¬ìš©ì ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ ì¼ì • ìƒì„± (allocate ì•¡ì…˜ ì‚¬ìš©)
    schedule_result = await schedule_tools.execute({
        "action": "allocate",
        "tasks": [
            {
                "id": "user_request_task",
                "title": title,
                "duration": 60,
                "priority": 8,
                "deadline": datetime.now().replace(hour=17, minute=0, second=0, microsecond=0).isoformat()
            }
        ],
        "constraints": {
            "working_hours": {"start": "09:00", "end": "18:00"},
            "break_times": ["12:00-13:00", "15:00-15:15"],
            "max_continuous_work": 120,
            "preferred_work_style": "focused_blocks"
        }
    })
    
    # ì‚¬ìš©ì ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ì¼ì • ìƒì„±
    tasks = []
    
    # AIê°€ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì‹œê°„ê³¼ ì†Œìš”ì‹œê°„ ê³„ì‚°
    time_analysis_prompt = f"""
    ì‚¬ìš©ì ìš”ì²­: "{pure_user_request}"
    
    ì´ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì¼ì • ì‹œê°„ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”:
    1. ì–¸ì œ ì‹œì‘í• ì§€ (ì˜ˆ: "ë‚´ì¼ ê°™ì€ ì‹œê°„" = ë‚´ì¼ ì˜¤í›„ 7ì‹œ, "ì˜¤ëŠ˜ 3ì‹œ" = ì˜¤ëŠ˜ ì˜¤í›„ 3ì‹œ)
    2. ì–¼ë§ˆë‚˜ ê±¸ë¦´ì§€ (ì˜ˆ: "1ì‹œê°„", "30ë¶„", ê¸°ë³¸ê°’ 60ë¶„)
    
    í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M')}
    
    JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
    {{
        "start_time": "YYYY-MM-DD HH:MM:SS",
        "duration_minutes": 60
    }}
    """
    
    time_analysis_result = llm.invoke(time_analysis_prompt)
    time_analysis_text = time_analysis_result.content.strip()
    
    # JSON íŒŒì‹± ì‹œë„
    try:
        import json
        if "{" in time_analysis_text and "}" in time_analysis_text:
            json_start = time_analysis_text.find("{")
            json_end = time_analysis_text.rfind("}") + 1
            json_text = time_analysis_text[json_start:json_end]
            time_data = json.loads(json_text)
            parsed_time = datetime.fromisoformat(time_data.get("start_time", datetime.now().isoformat()))
            parsed_duration = time_data.get("duration_minutes", 60)
        else:
            parsed_time = datetime.now() + timedelta(hours=1)
            parsed_duration = 60
    except:
        parsed_time = datetime.now() + timedelta(hours=1)
        parsed_duration = 60
    
    print(f"ì‚¬ìš©ì ìš”ì²­: {pure_user_request}")
    print(f"AIê°€ ê³„ì‚°í•œ ì‹œê°„: {parsed_time}")
    print(f"AIê°€ ê³„ì‚°í•œ ì†Œìš”ì‹œê°„: {parsed_duration}ë¶„")
    
    # AIê°€ ìƒì„±í•œ titleê³¼ description ì‚¬ìš©
    tasks.append({
        "id": f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "title": title,
        "duration": parsed_duration,
        "priority": 6,
        "deadline": parsed_time.isoformat()
    })
    
    # ì¼ì •ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    for task in tasks:
        save_result = await schedule_tools.execute({
            "action": "save_schedule",
            "user_id": user_id,
            "title": task["title"],
            "description": description,
            "start_time": parsed_time,
            "end_time": parsed_time + timedelta(minutes=task["duration"]),
            "duration": task["duration"],
            "priority": task["priority"]
        })
        logger.info(f"ì¼ì • ì €ì¥ ê²°ê³¼: {save_result}")
        
        # ì‚¬ìš©ìì—ê²Œ ì¼ì • ë“±ë¡ ì™„ë£Œ ì•Œë¦¼
        if save_result.get("status") == "success":
            print(f"\nâœ… ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“… ì œëª©: {task['title']}")
            print(f"â° ì‹œê°„: {parsed_time.strftime('%Y-%m-%d %H:%M')}")
            print(f"â±ï¸ ì†Œìš”ì‹œê°„: {task['duration']}ë¶„")
            print()
        else:
            print(f"\nâŒ ì¼ì • ë“±ë¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {save_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            print()
    
    # ì¼ì • ìµœì í™” (optimize ì•¡ì…˜ ì‚¬ìš©)
    optimization_result = await schedule_tools.execute({
        "action": "optimize",
        "schedule_id": schedule_id,
        "optimization_type": "efficiency"
    })
    
    # ì œì•½ì¡°ê±´
    constraints = {
        "working_hours": {"start": "09:00", "end": "18:00"},
        "break_times": ["12:15-13:15", "15:00-15:15"],
        "max_continuous_work": 120,  # ë¶„
        "preferred_work_style": "focused_blocks"
    }
    
    # íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚° (ë„êµ¬ ê²°ê³¼ ì‚¬ìš©)
    efficiency_score = optimization_result.get("efficiency_score", 85)
    
    # ì¶©ëŒ ê²€ì‚¬ (ë„êµ¬ ê²°ê³¼ ì‚¬ìš©)
    conflicts = optimization_result.get("conflicts", [])
    
    return {
        "schedule_id": schedule_id,
        "tasks": tasks,
        "time_blocks": schedule_result.get("schedule_blocks", []),
        "constraints": constraints,
        "efficiency_score": schedule_result.get("efficiency_score", 85),
        "conflicts": [],
        "schedule_result": schedule_result,
        "optimization_result": optimization_result
    }


def calculate_efficiency_score(time_blocks: list, constraints: dict) -> float:
    """
    ì¼ì •ì˜ íš¨ìœ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        time_blocks (list): ì‹œê°„ ë¸”ë¡ ëª©ë¡
        constraints (dict): ì œì•½ì¡°ê±´
        
    Returns:
        float: íš¨ìœ¨ì„± ì ìˆ˜ (0-100)
    """
    if not time_blocks:
        return 0.0
    
    # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
    total_priority = sum(block.get("priority", 0) for block in time_blocks)
    max_possible_priority = len(time_blocks) * 10
    
    efficiency = (total_priority / max_possible_priority) * 100 if max_possible_priority > 0 else 0
    
    # ì œì•½ì¡°ê±´ ìœ„ë°˜ ì‹œ ì ìˆ˜ ê°ì 
    working_hours = constraints.get("working_hours", {})
    work_start = datetime.strptime(working_hours.get("start", "09:00"), "%H:%M").time()
    work_end = datetime.strptime(working_hours.get("end", "18:00"), "%H:%M").time()
    
    for block in time_blocks:
        start_time = datetime.fromisoformat(block["start_time"]).time()
        end_time = datetime.fromisoformat(block["end_time"]).time()
        
        if start_time < work_start or end_time > work_end:
            efficiency -= 10  # ê·¼ë¬´ì‹œê°„ ì™¸ ì‘ì—… ì‹œ ê°ì 
    
    return max(0, min(100, efficiency))


def check_schedule_conflicts(time_blocks: list, constraints: dict) -> list:
    """
    ì¼ì • ì¶©ëŒì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
    
    Args:
        time_blocks (list): ì‹œê°„ ë¸”ë¡ ëª©ë¡
        constraints (dict): ì œì•½ì¡°ê±´
        
    Returns:
        list: ì¶©ëŒ ëª©ë¡
    """
    conflicts = []
    
    # ì‹œê°„ ë¸”ë¡ ê°„ ì¶©ëŒ ê²€ì‚¬
    for i, block1 in enumerate(time_blocks):
        for j, block2 in enumerate(time_blocks[i+1:], i+1):
            start1 = datetime.fromisoformat(block1["start_time"])
            end1 = datetime.fromisoformat(block1["end_time"])
            start2 = datetime.fromisoformat(block2["start_time"])
            end2 = datetime.fromisoformat(block2["end_time"])
            
            if start1 < end2 and start2 < end1:
                conflicts.append(f"ì‹œê°„ ì¶©ëŒ: {block1['title']}ê³¼ {block2['title']}")
    
    # ì œì•½ì¡°ê±´ ìœ„ë°˜ ê²€ì‚¬
    max_continuous = constraints.get("max_continuous_work", 120)
    current_continuous = 0
    
    for block in time_blocks:
        duration = block["duration"]
        current_continuous += duration
        
        if current_continuous > max_continuous:
            conflicts.append(f"ì—°ì† ì‘ì—… ì‹œê°„ ì´ˆê³¼: {current_continuous}ë¶„")
            current_continuous = 0
        else:
            current_continuous = 0  # íœ´ì‹ ì‹œê°„ìœ¼ë¡œ ë¦¬ì…‹
    
    return conflicts




async def generate_plan_recommendation(schedule_plan: Dict[str, Any], state: State) -> str:
    """
    ì¼ì • ê³„íš ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ AI ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        schedule_plan (Dict[str, Any]): ì¼ì • ê³„íš ê²°ê³¼
        state (State): í˜„ì¬ ìƒíƒœ
        
    Returns:
        str: AI ì¶”ì²œ ë©”ì‹œì§€
    """
    try:
        from langchain_openai import ChatOpenAI
        import os
        
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            api_key=os.getenv("OPENAI_API_KEY"),
            temperature=0.7,
            streaming=True  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ í™œì„±í™”
        )
        
        # ì´ì „ ëŒ€í™” ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        messages = state.get("messages", [])
        recent_messages = messages[-5:] if len(messages) > 5 else messages
        conversation_context = ""
        for msg in recent_messages:
            if msg.get("role") == "user":
                conversation_context += f"ì‚¬ìš©ì: {msg.get('content', '')}\n"
            elif msg.get("role") == "assistant":
                conversation_context += f"AI: {msg.get('content', '')}\n"
        
        # ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        health_data = state.get("health_data")
        worklife_data = state.get("worklife_data")
        
        prompt = f"""
        ë‹¹ì‹ ì€ Plandy AIì˜ Plan Agentì…ë‹ˆë‹¤.
        
        ì‚¬ìš©ì ìš”ì²­: {state.get('user_request', '')}
        
        ì´ì „ ëŒ€í™” ë‚´ìš©:
        {conversation_context}
        
        ì¼ì • ê³„íš ê²°ê³¼:
        - íš¨ìœ¨ì„± ì ìˆ˜: {schedule_plan['efficiency_score']}/100
        - ì¶©ëŒ ê°œìˆ˜: {len(schedule_plan['conflicts'])}
        - ì‹œê°„ ë¸”ë¡ ìˆ˜: {len(schedule_plan['time_blocks'])}
        
        ì‹œê°„ ë¸”ë¡: {schedule_plan['time_blocks']}
        ì¶©ëŒ ëª©ë¡: {schedule_plan['conflicts']}
        ì œì•½ì¡°ê±´: {schedule_plan['constraints']}
        
        ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì˜ ë°ì´í„°:
        - ê±´ê°• ë°ì´í„°: {health_data}
        - ì›Œë¼ë²¨ ë°ì´í„°: {worklife_data}
        
        ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³ , í•„ìš”í•˜ë‹¤ë©´ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
        ì˜ˆë¥¼ ë“¤ì–´:
        - Health Agentì—ê²Œ: ì¼ì •ì— ë”°ë¥¸ ê±´ê°• ê´€ë¦¬ ê¶Œì¥ì‚¬í•­
        - WorkLife Agentì—ê²Œ: ì¼ì •ì— ë”°ë¥¸ ì›Œë¼ë²¨ ê· í˜• ì¡°ì • ë°©ì•ˆ
        - Communication Agentì—ê²Œ: ì¼ì • ê´€ë ¨ ì‚¬ìš©ì ì‘ë‹µ ê°€ì´ë“œ
        
        ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ì„œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¼ì • ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
        """
        
        # ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ ìƒì„± (APIì—ì„œ ì‹¤ì‹œê°„ ì²˜ë¦¬)
        full_response = ""
        async for chunk in llm.astream(prompt):
            if hasattr(chunk, 'content') and chunk.content:
                full_response += chunk.content
        
        return full_response
    except Exception as e:
        # í´ë°±: ê¸°ë³¸ ì¶”ì²œ
        efficiency_score = schedule_plan["efficiency_score"]
        conflicts = schedule_plan["conflicts"]
        
        if efficiency_score >= 80 and not conflicts:
            recommendation = "ì¼ì •ì´ íš¨ìœ¨ì ìœ¼ë¡œ ê³„íšë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ê³„íšì„ ë”°ë¥´ì„¸ìš”."
        elif efficiency_score >= 60:
            recommendation = "ì¼ì •ì´ ì–‘í˜¸í•˜ì§€ë§Œ ëª‡ ê°€ì§€ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤."
        else:
            recommendation = "ì¼ì • ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. ìš°ì„ ìˆœìœ„ë¥¼ ì¬ê²€í† í•´ë³´ì„¸ìš”."
        
        if conflicts:
            recommendation += f" {len(conflicts)}ê°œì˜ ì¶©ëŒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        return f"{recommendation} (API ì˜¤ë¥˜: {str(e)})"
